import dotenv from "dotenv";
dotenv.config();

import { ReactAgentBuilder } from "../core";

const GEMINI_KEY = process.env.GEMINI_KEY;
const OPENAI_KEY = process.env.OPENAI_KEY;

/**
 * Example demonstrating image analysis capabilities with DelReact
 * This example shows how to use the new multimodal features
 */
async function exampleImageAnalysis() {
  console.log("ðŸ–¼ï¸ DelReact Image Analysis Example");
  console.log("=" .repeat(50));

  const agentBuilder = new ReactAgentBuilder({
    geminiKey: GEMINI_KEY, 
    openaiKey: OPENAI_KEY,
    useEnhancedPrompt: true // Enhanced prompt helps with image analysis
  });

  const agent = agentBuilder.init({
    selectedProvider: 'gemini', // Gemini has excellent vision capabilities
    model: 'gemini-2.5-flash',
  }).build();

  try {
    // Example 1: Direct LLM call with image
    console.log("\nðŸ” Example 1: Direct LLM call with image");
    
    const visionResult = await agentBuilder.callLLM(
      "Analyze this chart and provide key insights about the data trends",
      {
        provider: 'gemini',
        model: 'gemini-2.5-flash',
        images: [
          {
            data: "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mP8/5+hHgAHggJ/PchI7wAAAABJRU5ErkJggg==",
            detail: 'high'
          }
        ]
      }
    );
    
    console.log("Direct LLM result:", visionResult.substring(0, 100) + "...");

    // Example 2: Full agent workflow with images
    console.log("\nðŸ¤– Example 2: Full agent workflow with multimodal input");
    
    const result = await agent.invoke({
      objective: "Analyze the provided images and create a comprehensive report",
      outputInstruction: "Provide structured analysis including: visual description, key elements identified, data insights (if applicable), and actionable recommendations",
      images: [
        {
          data: "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mP8/5+hHgAHggJ/PchI7wAAAABJRU5ErkJggg==", // Red pixel
          detail: 'high'
        },
        {
          data: Buffer.from("fake-image-data"), // Example buffer
          mimeType: 'image/jpeg',
          detail: 'auto'
        }
      ]
    });

    console.log("\nâœ… Agent workflow completed");
    console.log(`ðŸ“„ Session ID: ${result.sessionId}`);
    console.log(`ðŸ“Š Conclusion: ${result.conclusion}`);
    console.log(`ðŸ”„ Tasks executed: ${result.fullState.actionedTasks.length}`);

    // Example 3: Business use case - Document analysis
    console.log("\nðŸ“‹ Example 3: Business document analysis");
    
    const docAnalysis = await agent.invoke({
      objective: "Extract key information from business documents and create an executive summary",
      outputInstruction: "Format as: Executive Summary, Key Metrics, Action Items, Risk Assessment",
      images: [
        {
          // In real usage, this would be a actual document image
          data: "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mP8/5+hHgAHggJ/PchI7wAAAABJRU5ErkJggg==",
          detail: 'high'
        }
      ]
    });

    console.log(`ðŸ“ˆ Document analysis completed: ${docAnalysis.conclusion.substring(0, 150)}...`);

  } catch (error: any) {
    console.error("âŒ Example failed:", error.message);
    
    // Check if it's an API key issue
    if (error.message.includes("API key")) {
      console.log("\nðŸ’¡ Note: Make sure to set your API keys in the .env file:");
      console.log("GEMINI_KEY=your_gemini_api_key");
      console.log("OPENAI_KEY=your_openai_api_key");
    }
  }
}

/**
 * Supported image formats and best practices
 */
function showImageSupportInfo() {
  console.log("\nðŸ“– Image Support Information");
  console.log("=" .repeat(40));
  
  console.log("\nðŸ–¼ï¸ Supported Input Formats:");
  console.log("â€¢ File paths: '/path/to/image.jpg'");
  console.log("â€¢ Data URLs: 'data:image/jpeg;base64,/9j/4AAQ...'");
  console.log("â€¢ Base64 strings: '/9j/4AAQSkZJRgABAQEASABIAAD...'");
  console.log("â€¢ Buffers: Buffer.from(imageData)");
  
  console.log("\nðŸŽ¯ Image Detail Levels:");
  console.log("â€¢ 'low': Fast processing, lower quality");
  console.log("â€¢ 'auto': Balanced processing (default)");
  console.log("â€¢ 'high': Detailed analysis, slower processing");
  
  console.log("\nðŸ“Š Supported MIME Types:");
  console.log("â€¢ image/jpeg, image/jpg");
  console.log("â€¢ image/png");
  console.log("â€¢ image/gif");
  console.log("â€¢ image/webp");
  console.log("â€¢ image/bmp");
  console.log("â€¢ image/svg+xml");
  
  console.log("\nðŸš€ Best Practices:");
  console.log("â€¢ Use 'high' detail for charts, diagrams, and text-heavy images");
  console.log("â€¢ Use 'auto' or 'low' for simple object recognition");
  console.log("â€¢ Optimize image size before processing (max 20MB recommended)");
  console.log("â€¢ Provide clear, high-contrast images for better analysis");
  console.log("â€¢ Use vision-capable models like 'gemini-2.5-flash' or 'gpt-4o-mini'");
}

const main = async () => {
  showImageSupportInfo();
  
  if (GEMINI_KEY || OPENAI_KEY) {
    await exampleImageAnalysis();
  } else {
    console.log("\nâš ï¸ No API keys found. Set GEMINI_KEY or OPENAI_KEY to run examples.");
  }
};

main().catch(error => {
  console.error("Error running examples:", error);
});